---
title: "Platform Risk Control Vulnerability"
date: 2024-01-01
categories: ["Tech Projects"]
---

## *The Automated Reporting Mechanism Behind Bilibili’s Recent Wave of User Bans*

## Background
In April 2025, a large number of Bilibili users were banned despite showing no clear violations of the platform’s rules. The official platform provided no specific explanation. Many users suspected the bans were linked to recent updates in Bilibili’s AI-powered content moderation system, which may have mistakenly flagged normal users.
However, based on feedback from banned users, I personally believe that these bans may not be isolated incidents. Instead, they likely reflect a systemic issue in which individuals exploited flaws in Bilibili’s new review mechanism by combining mass reporting with automated processing, leading to malicious bans.
As someone currently studying the fundamentals of IT, I’ve attempted to reconstruct a possible attack flow from a technical perspective.

## Analyzing the Possibility of Automated Malicious Reporting
### Operation Flow and Technical Means
These attacks were likely carried out using automated scripts combined with large volumes of fake accounts. The technical barrier to implement such actions is relatively low:
Acquiring large quantities of fake accounts through registration or purchase (commonly featuring: stock profile images, zero followers, and no posts)
Using Python scripts in combination with browser automation tools like Selenium or Playwright to log in and operate these accounts
Continuously monitoring the homepage activity of selected target users, then triggering mass reports as soon as new content is detected
### How Are Victims Selected?
**Marking Targets**
Usernames and past controversial comments are recorded
Fake accounts follow you pretending to be fans, while actually monitoring your homepage
Scripts regularly scan the user’s “Following” list to detect and react to new activity
**Report Trigger Mechanism**
Dozens of small accounts simultaneously report a user’s previous comments or newly posted content
Once the report threshold is reached, the system may ban the user automatically without manual review
**Platform Risk Control Weaknesses**
Bilibili’s content moderation likely follows a “threshold-based + AI moderation” logic: once the number of reports crosses a certain limit, the system bans users directly
Under this logic, attackers using dense reporting scripts can achieve precise targeting and auto-banning, which leaves the system vulnerable to abuse

## How to Avoid Being Targeted or Mass Reported
### Identify and Block Suspicious Followers
If you notice many strange accounts suddenly following you (no interaction, no posts, stock profile pictures), they are likely part of an automated attack system
Block these accounts as soon as possible to reduce the chance of your homepage being monitored
### Disrupt Script Recognition Paths
Change your username, update your profile picture, and delete any previously controversial comments
Reduce new content posting frequency in the short term to avoid being detected by auto-scanning scripts

## The Technical Implementation Is Actually Quite Basic Although harmful in effect, the methods used are technically simple:
- Tools: Python + Selenium / Playwright
- Workflow: Auto login → Scan following list → Detect new activity → Execute reports
- Account source: Bulk-purchased or registered throwaway accounts
- Detection method: Simulated browser actions or direct scraping of website APIs
As a beginner in IT, I realized this kind of script could be written and deployed in a relatively short time. This only highlights that the real issue isn’t the technology itself, but rather the flawed design of the platform’s moderation system.

## The Root Cause Lies in the Platform
Users can take measures to protect themselves, but the fundamental issue lies with Bilibili’s non-transparent moderation mechanism and the unreasonable automatic banning system.
Even after users voiced concerns across multiple channels, the platform has yet to publicly respond or propose any fixes.
This analysis has helped me realize that technology itself is not dangerous — what’s truly harmful is when poorly designed systems allow technology to be abused without accountability.
